import torch
# from timesformer_pytorch import TimeSformer
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

LOG_SIG_MAX = 2
LOG_SIG_MIN = -20
epsilon = 1e-6

class ResidualBlock(nn.Module):
    """
    åŸºç¡€æ®‹å·®å—ï¼ŒåŒ…å«ä¸¤ä¸ª3x3å·ç§¯å±‚ã€‚
    
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        # nn.Conv2dç”¨äºæ‰§è¡ŒäºŒç»´å·ç§¯æ“ä½œ
        # å·ç§¯æ ¸ä¸æ„Ÿå—é‡å†…çš„å€¼è¿›è¡ŒçŸ©é˜µç›¸ä¹˜å¹¶æ±‚å’Œï¼Œè¾“å‡ºä¸€ä¸ªå€¼
        # in_channels: è¾“å…¥å›¾åƒçš„é€šé“æ•°ã€‚å¯¹äºç°åº¦å›¾åƒï¼Œin_channels ä¸º 1ã€‚å¯¹äºRGBå›¾åƒï¼Œin_channels ä¸º 3ã€‚
            # å¦‚æœä½ çš„è¾“å…¥æ˜¯ä¸Šä¸€å±‚å·ç§¯çš„è¾“å‡ºï¼Œé‚£ä¹ˆ in_channels å°±æ˜¯ä¸Šä¸€å±‚çš„ out_channels
        # out_channels: å·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾å›¾çš„æ•°é‡ï¼Œä¹Ÿå°±æ˜¯å·ç§¯æ ¸ï¼ˆæˆ–æ»¤æ³¢å™¨ï¼‰çš„æ•°é‡
        # kernel_size: å·ç§¯æ ¸ï¼ˆæˆ–æ»¤æ³¢å™¨ï¼‰çš„å¤§å°ã€‚è®¾ç½®ä¸º 3ï¼Œè¡¨ç¤ºå·ç§¯æ ¸æ˜¯ä¸€ä¸ª 3x3 çš„æ­£æ–¹å½¢ã€‚
            # ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå…ƒç»„æ¥æŒ‡å®šéæ­£æ–¹å½¢çš„å·ç§¯æ ¸ï¼Œä¾‹å¦‚ (3, 5) è¡¨ç¤º 3 è¡Œ 5 åˆ—çš„å·ç§¯æ ¸ã€‚
        # stride: å·ç§¯æ ¸åœ¨è¾“å…¥ç‰¹å¾å›¾ä¸Šæ»‘åŠ¨çš„æ­¥é•¿
            # stride=1 (é»˜è®¤å€¼)ï¼Œå·ç§¯æ ¸æ¯æ¬¡ç§»åŠ¨ä¸€ä¸ªåƒç´ 
            # stride=2ï¼Œå·ç§¯æ ¸æ¯æ¬¡ç§»åŠ¨ä¸¤ä¸ªåƒç´ ï¼Œå¯¼è‡´è¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸å‡åŠï¼Œå¸¸ç”¨äºé™é‡‡æ ·
            # å¯ä»¥æŒ‡å®šä¸€ä¸ªå…ƒç»„ (å¦‚ stride=(1, 2))ï¼Œè¡¨ç¤ºæ°´å¹³å’Œå‚ç›´æ–¹å‘çš„æ­¥é•¿ä¸åŒ
        # padding: åœ¨è¾“å…¥ç‰¹å¾å›¾çš„è¾¹ç•Œå‘¨å›´æ·»åŠ çš„é›¶çš„æ•°é‡
            # ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†åœ¨å·ç§¯æ“ä½œä¸­ä¿ç•™è¾“å…¥ç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸ï¼Œé˜²æ­¢è¾¹ç¼˜ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ä½¿å¾—è¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ä¸è¾“å…¥ç‰¹å¾å›¾æ›´æ¥è¿‘æˆ–ç›¸åŒ
        # bias: ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨å·ç§¯æ“ä½œåæ·»åŠ åç½®
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        # åˆ›å»ºä¸€ä¸ªäºŒç»´æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰å±‚
        # nn.BatchNorm2d å±‚ç´§è·Ÿåœ¨ nn.Conv2d å±‚ä¹‹åï¼Œnum_featuresåº”è¯¥ä¸å‰é¢ nn.Conv2d å±‚çš„ out_channels ç›¸åŒ¹é…
        # BatchNorm2d å±‚ä¼šå¯¹è¾“å…¥æ•°æ®çš„æ¯ä¸ªé€šé“ç‹¬ç«‹åœ°è¿›è¡Œå½’ä¸€åŒ–æ“ä½œã€‚
        # å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼ˆmini-batchï¼‰çš„è¾“å…¥æ•°æ®ï¼Œå®ƒä¼šè®¡ç®—æ¯ä¸ªé€šé“çš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åä½¿ç”¨è¿™äº›ç»Ÿè®¡é‡æ¥å½’ä¸€åŒ–è¯¥é€šé“çš„æ•°æ®ï¼Œä½¿å…¶å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ã€‚
        # å®ƒè¿˜ä¼šå­¦ä¹ ä¸¤ä¸ªå¯è®­ç»ƒçš„å‚æ•°ï¼šç¼©æ”¾å› å­ğ›¾(gamma)å’Œåç§»å› å­ğ›½(beta)ï¼Œç”¨æ¥å¯¹å½’ä¸€åŒ–åçš„æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä»¥æ¢å¤ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        # å®šä¹‰ä¸€ä¸ªæ®‹å·®è·³è·ƒè¿æ¥ï¼ˆshortcut connectionï¼‰
        self.shortcut = nn.Sequential() # ä¸€ä¸ªç©ºçš„ nn.Sequential()èµ·åˆ°æ’ç­‰æ˜ å°„çš„ä½œç”¨ï¼Œå®ƒå°†è¾“å…¥ç›´æ¥ä¼ é€’åˆ°è¾“å‡º
        if stride != 1 or in_channels != out_channels: # ä¸ºäº†ä½¿è·³è·ƒè¿æ¥çš„è¾“å‡ºå°ºå¯¸ä¸ä¸»è·¯å¾„çš„è¾“å‡ºå°ºå¯¸åŒ¹é…ï¼Œè·³è·ƒè¿æ¥æœ¬èº«ä¹Ÿéœ€è¦è¿›è¡Œç›¸åº”çš„ç©ºé—´é™é‡‡æ ·
            self.shortcut = nn.Sequential(
                # 1x1 å·ç§¯å±‚ï¼ˆä¹Ÿç§°ä¸ºé€ç‚¹å·ç§¯ï¼‰ï¼Œä¸»è¦ä½œç”¨ä¸æ˜¯æå–ç©ºé—´ç‰¹å¾ï¼Œè€Œæ˜¯ç”¨æ¥æ”¹å˜ç‰¹å¾å›¾çš„é€šé“æ•° (in_channels å˜ä¸º out_channels)
                # strideä¸ if æ¡ä»¶ä¸­çš„ stride ä¿æŒä¸€è‡´ï¼Œå¦‚æœä¸»è·¯å¾„è¿›è¡Œäº†ç©ºé—´é™é‡‡æ ·ï¼Œ1x1å·ç§¯ä¹Ÿä¼šæ‰§è¡Œç›¸åŒçš„é™é‡‡æ ·ï¼Œç¡®ä¿è·³è·ƒè¿æ¥çš„è¾“å‡ºç©ºé—´å°ºå¯¸ä¸ä¸»è·¯å¾„çš„è¾“å‡ºåŒ¹é…
                # è¿™é‡Œä¸è¿›è¡Œpaddingæ˜¯åŒ¹é…ä¸»è·¯çš„kernel=3ï¼Œå¦‚æœkernelè¾ƒå¤§æ—¶åœ¨è¿™é‡Œä¹Ÿéœ€è¦å¤„ç†paddingä»¥åŒ¹é…ä¸»è·¯ç‰¹å¾å›¾
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False), 
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.leaky_relu(self.bn1(self.conv1(x))) # å¥½åƒleakyreluä¼šå¥½ä¸€ç‚¹
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.leaky_relu(out)
        return out

class ResNet(nn.Module):
    """è‡ªå®šä¹‰ResNetï¼Œä¸»è¾“å‡ºä¸ºç‰¹å¾å‘é‡ï¼Œå¹¶å¸¦æœ‰ä¸€ä¸ªç”¨äºé¢„æµ‹ä½å§¿çš„è¾…åŠ©å¤´ã€‚"""
    def __init__(self, num_aux_outputs, input_channels=3): # ä¸ç”¨æŒ‡å®šåƒç´ ï¼ŒåªæŒ‡å®šé€šé“æ•°å°±è¡Œ
        super(ResNet, self).__init__()
        # å‚è€ƒçœŸÂ·ResNetï¼Œç¬¬ä¸€å±‚æ˜¯size=7çš„å·ç§¯æ ¸ï¼Œpaddingä¸º3ï¼Œä½†æ˜¯è¿™æ ·è¾“å‡ºçš„ç‰¹å¾å›¾å°ºå¯¸æ˜¯å–å†³äºxå¥‡å¶æ€§çš„(x+1)/2
        # ä¸€èˆ¬éƒ½æ˜¯å¥‡æ•°å¤§å°å·ç§¯æ ¸ï¼Œæœ‰ä¸ªæ˜ç¡®çš„ä¸­å¿ƒï¼Œæ‰€ä»¥stride=2çš„æƒ…å†µä¸‹è¾“å‡ºçš„å›¾åƒå°ºå¯¸ä¸€å®šä¸ç¡®å®š
        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        # å®šä¹‰ä¸€ä¸ªäºŒç»´æœ€å¤§æ± åŒ–å±‚ï¼Œé€šè¿‡åœ¨ä¸€ä¸ªå±€éƒ¨åŒºåŸŸï¼ˆç”± kernel_size å®šä¹‰ï¼‰å†…å–æœ€å¤§å€¼æ¥å¯¹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œä¸‹é‡‡æ ·ï¼ˆé™é‡‡æ ·ï¼‰
            # é™ä½ç»´åº¦ï¼šå‡å°‘ç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸ï¼Œä»è€Œå‡å°‘åç»­å±‚çš„è®¡ç®—é‡å’Œå‚æ•°æ•°é‡
            # æå–ä¸»è¦ç‰¹å¾ï¼šä¿ç•™å±€éƒ¨åŒºåŸŸå†…æœ€æ˜¾è‘—çš„ç‰¹å¾ï¼ˆæœ€å¤§å€¼ï¼‰ï¼Œå¿½ç•¥ä¸é‡è¦çš„ç»†èŠ‚
            # å¢å¼ºå¹³ç§»ä¸å˜æ€§ï¼šå³ä½¿è¾“å…¥ä¸­çš„ç‰¹å¾å‘ç”Ÿäº†è½»å¾®çš„å¹³ç§»ï¼Œç”±äºå–æœ€å¤§å€¼çš„æ“ä½œï¼Œè¾“å‡ºç‰¹å¾ä¹Ÿå¯èƒ½ä¿æŒä¸å˜ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹å¯¹ç‰¹å¾çš„ä½ç½®ä¸é‚£ä¹ˆæ•æ„Ÿ
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.block1 = ResidualBlock(64, 64, stride=1)
        # ä¸æ–­é€šè¿‡stride=2ä¸‹é‡‡æ ·ï¼Œç¼©å°ç‰¹å¾å›¾çš„å°ºå¯¸åŒæ—¶å¢åŠ ç‰¹å¾å›¾çš„é€šé“æ•°
        # æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œä¸­éå¸¸å¸¸è§çš„æ¨¡å¼ï¼Œç”¨äºåœ¨ç½‘ç»œæ·±å±‚æå–æ›´é«˜çº§ã€æ›´æŠ½è±¡çš„ç‰¹å¾ï¼ŒåŒæ—¶å‡å°‘ç©ºé—´ç»´åº¦ä»¥èŠ‚çœè®¡ç®—é‡å’Œå‚æ•°
        self.block2 = ResidualBlock(64, 128, stride=2)
        self.block3 = ResidualBlock(128, 256, stride=2)
        self.block4 = ResidualBlock(256, 512, stride=2)

        # äºŒç»´è‡ªé€‚åº”å¹³å‡æ± åŒ–å±‚ï¼ŒæŒ‡å®šçš„æ˜¯ç›®æ ‡è¾“å‡ºå°ºå¯¸ï¼Œè€Œä¸æ˜¯æ ¸å¤§å°å’Œæ­¥é•¿ã€‚
        # ç½‘ç»œä¼šæ ¹æ®è¾“å…¥ç‰¹å¾å›¾çš„å°ºå¯¸ï¼Œè‡ªåŠ¨è®¡ç®—å‡ºåˆé€‚çš„ kernel_size å’Œ stride æ¥è¾¾åˆ°æ‚¨æŒ‡å®šçš„ç›®æ ‡è¾“å‡ºå°ºå¯¸
        # è®¾ç½® output_size=(1, 1) æ—¶ï¼Œnn.AdaptiveAvgPool2dä¼šå–è¾“å…¥ç‰¹å¾å›¾çš„æ‰€æœ‰åƒç´ çš„å¹³å‡å€¼ï¼Œä¸ºæ¯ä¸ªé€šé“ç”Ÿæˆä¸€ä¸ªå•ä¸€çš„å€¼
        # ç”¨æ¥æ›¿ä»£ä¼ ç»Ÿçš„ã€åœ¨å·ç§¯å±‚ä¹‹åä½¿ç”¨çš„å…¨è¿æ¥å±‚
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        # æ˜¾å¼è¾“å‡ºç›¸å¯¹ä½å§¿çš„è¾…åŠ©è¾“å‡ºå¤´
        # nn.Flattenå°†è¾“å…¥çš„å¤šç»´å¼ é‡ï¼ˆTensorï¼‰å±•å¹³ï¼ˆflattenï¼‰æˆä¸€ç»´å¼ é‡
        # ä¿ç•™ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œé€šå¸¸æ˜¯æ‰¹é‡å¤§å°ï¼Œç„¶åå°†æ‰€æœ‰åç»­ç»´åº¦ï¼ˆé€šé“ã€é«˜åº¦ã€å®½åº¦ç­‰ï¼‰åˆå¹¶ï¼ˆæˆ–å±•å¹³ï¼‰æˆä¸€ä¸ªå•ä¸€çš„ç»´åº¦
        # è¿™é‡Œæ˜¯(batch_size, 512,1,1)è¢«è½¬æˆ(batch_size,512)
        self.aux_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.Linear(512, 256), nn.ReLU(),
            nn.Linear(256, num_aux_outputs)
        )

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        aux_output = self.aux_head(x) # ç¬¬å››ä¸ªæ®‹å·®å—å‡ºæ¥å°±ç›´æ¥å»è¾…åŠ©å¤´äº†
        main_features = self.avgpool(x) # ç¬¬å››ä¸ªæ®‹å·®å—å‡ºæ¥ç»è¿‡å¹³å‡æ± åŒ–å½¢æˆä¸»ç‰¹å¾å‘é‡
        main_features = torch.flatten(main_features, 1)
        return main_features, aux_output # è¿”å›ä¸»ç‰¹å¾å’Œè¾…åŠ©å¤´çš„æ˜¾å¼è¾“å‡º

class GRU(nn.Module):
    """
    é›†æˆäº†ResNetå’ŒGRUçš„æ¨¡å‹ã€‚
    - ResNetæå–ç©ºé—´ç‰¹å¾ï¼Œå¹¶æœ‰è¾…åŠ©å¤´é¢„æµ‹å§¿æ€/ä½ç½®ã€‚
    - GRUå¤„ç†æ—¶åºä¿¡æ¯ï¼Œå¹¶æœ‰è¾…åŠ©å¤´é¢„æµ‹é€Ÿåº¦/è§’é€Ÿåº¦ã€‚
    - æœ€ç»ˆè¾“å‡ºä¸€ä¸ªèåˆæ—¶ç©ºä¿¡æ¯çš„ç‰¹å¾å‘é‡ã€‚
    """
    def __init__(self, resnet_aux_outputs, gru_hidden_dim, gru_aux_outputs, gru_layers=2, dropout=0.3):
        """
        Args:
            resnet_aux_outputs (int): ResNetè¾…åŠ©å¤´è¾“å‡ºç»´åº¦ (ä¾‹å¦‚: 6ä¸ªä½å§¿å‚æ•°)
            gru_hidden_dim (int): GRUéšè—å±‚ç»´åº¦ï¼ˆç‰¹å¾å‘é‡ç»´åº¦ï¼‰
            gru_aux_outputs (int): GRUè¾…åŠ©å¤´è¾“å‡ºæ•°é‡ (ä¾‹å¦‚: 6ä¸ªé€Ÿåº¦/è§’é€Ÿåº¦å‚æ•°)
        """
        super(GRU, self).__init__()
        
        # ResNeté€å¸§æå–ç‰¹å¾
        self.image_feature_extractor = ResNet(num_aux_outputs=resnet_aux_outputs)
        resnet_main_feature_dim = 512 # ResNetçš„ä¸»è¾“å‡ºç»´åº¦
        
        # GRUçš„è¾“å…¥ç»´åº¦ = å›¾åƒä¸»ç‰¹å¾ + å¤–éƒ¨åŠ¨æ€ç‰¹å¾ï¼Œæš‚æ—¶å…ˆåªæœ‰å›¾åƒ
        gru_input_dim = resnet_main_feature_dim # + external_dynamic_features
        
        # GRUå¤„ç†æ—¶åºè¾“å‡ºæ—¶åºä¿¡æ¯
        # input_sizeæ˜¯è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³å¯¹äºåºåˆ—ä¸­çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè¾“å…¥åˆ° GRU å•å…ƒçš„æ•°æ®çš„ç‰¹å¾æ•°é‡
        # hidden_sizeæ˜¯éšè—çŠ¶æ€ (hidden state) çš„ç»´åº¦ã€‚
            # GRU å•å…ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—å¹¶æ›´æ–°ä¸€ä¸ªéšè—çŠ¶æ€ï¼Œhidden_size å®šä¹‰äº†è¿™ä¸ªéšè—çŠ¶æ€å‘é‡çš„é•¿åº¦
        # num_layersæ˜¯å †å çš„ GRU å±‚æ•°ã€‚
            # å¦‚æœ num_layers > 1ï¼Œé‚£ä¹ˆ GRU ç½‘ç»œå°†ç”±å¤šä¸ª GRU å±‚å †å è€Œæˆã€‚
            # ç¬¬ä¸€ä¸ª GRU å±‚çš„è¾“å…¥æ˜¯åŸå§‹åºåˆ—æ•°æ®ã€‚éšåçš„æ¯ä¸ª GRU å±‚çš„è¾“å…¥æ˜¯å‰ä¸€ä¸ª GRU å±‚çš„è¾“å‡ºåºåˆ—ã€‚
            # è¿™ç§å †å ç»“æ„å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¤æ‚ã€æ›´é«˜å±‚æ¬¡çš„æ—¶é—´ä¾èµ–å…³ç³»
        # batch_firstæ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œç”¨äºæŒ‡å®šè¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„ç»´åº¦é¡ºåºã€‚
            # batch_first=Trueï¼Œé‚£ä¹ˆè¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶å°†æ˜¯ (batch, seq_len, features)
        # dropout é™¤æœ€åä¸€å±‚ä¹‹å¤–çš„ GRU å±‚è¾“å‡ºçš„ Dropout æ¦‚ç‡ã€‚
            # Dropout æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šéšæœºåœ°â€œå…³é—­â€ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºã€‚
            # Dropout åªåº”ç”¨äºå †å  GRU å±‚ä¹‹é—´çš„è¿æ¥ï¼Œè€Œä¸ä¼šåº”ç”¨äº GRU å•å…ƒå†…éƒ¨çš„å¾ªç¯è¿æ¥ã€‚
        # ã€å…³äºGRUçš„ä¸¤ä¸ªé—¨ã€‘PyTorch ä¼šè‡ªåŠ¨åœ¨å†…éƒ¨åˆ›å»ºå®ç°è¿™ä¸¤ä¸ªé—¨æ‰€éœ€çš„æ‰€æœ‰æƒé‡çŸ©é˜µå’Œåç½®é¡¹
        self.gru = nn.GRU(
            input_size=gru_input_dim,
            hidden_size=gru_hidden_dim,
            num_layers=gru_layers,
            batch_first=True,
            dropout=dropout if gru_layers > 1 else 0
        )
        
        # GRUçš„è¾…åŠ©å¤´: ç”¨äºæ˜¾å¼é¢„æµ‹é€Ÿåº¦/è§’é€Ÿåº¦
        # å®ƒä½œç”¨äºGRUçš„æ•´ä¸ªè¾“å‡ºåºåˆ—ï¼Œä»¥å¾—åˆ°æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
        self.gru_aux_head = nn.Linear(gru_hidden_dim, gru_aux_outputs)

    def forward(self, image_sequence):
        """
        Args:
            image_sequence (Tensor): å½¢çŠ¶ä¸º (Batchæ‰¹é‡å¤§å°, Timeå¸§æ•°, Channelsé€šé“æ•°, Heighté«˜åº¦, Widthå®½åº¦) çš„å›¾åƒåºåˆ—
        
        è¿”å›:
            ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ï¼š
            - final_feature_vector (Tensor): GRUæœ€åçš„éšè—çŠ¶æ€, å½¢çŠ¶ä¸º (B, H_gru)
            - resnet_aux_predictions (Tensor): ResNetçš„å§¿æ€é¢„æµ‹, å½¢çŠ¶ä¸º (B, T, F_pose)
            - gru_aux_predictions (Tensor): GRUçš„é€Ÿåº¦é¢„æµ‹, å½¢çŠ¶ä¸º (B, T, F_vel)
        """
        # åŸå§‹è¾“å…¥å½¢çŠ¶:
        # image_sequence: (B, T, C, H, W)  (B=æ‰¹é‡å¤§å°, T=å¸§æ•°, C=é€šé“æ•°, H=é«˜åº¦, W=å®½åº¦)
        B, T, C, H, W = image_sequence.shape

        # å°†æ—¶é—´å’Œæ‰¹æ¬¡ç»´åº¦â€œå‹å¹³â€ (Flatten/Reshape)
        # å°† (B, T, C, H, W) -> (B * T, C, H, W)
        # è®©ResNetä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰åºåˆ—ä¸­çš„æ‰€æœ‰å¸§ï¼Œå¦‚åŒä¸€ä¸ªè¶…å¤§çš„batch
        input_for_resnet = image_sequence.view(B * T, C, H, W)

        # ä¸€æ¬¡æ€§é€šè¿‡ç‰¹å¾æå–å™¨ (Single Forward Pass)
        # è¿™æ˜¯å…³é”®çš„å¹¶è¡ŒåŒ–æ­¥éª¤ã€‚GPUä¼šå¹¶è¡Œå¤„ç†è¿™ B*T å¼ å›¾ç‰‡ã€‚
        # resnet_main_feat çš„å½¢çŠ¶ä¼šæ˜¯ (B * T, feat_dim)
        # resnet_aux_pred çš„å½¢çŠ¶ä¼šæ˜¯ (B * T, 6)  (å‡è®¾6Då§¿æ€)
        resnet_main_feat, resnet_aux_pred = self.image_feature_extractor(input_for_resnet)

        # æ¢å¤æ—¶é—´å’Œæ‰¹æ¬¡ç»´åº¦ (Unflatten/Reshape)ï¼Œå°†ResNetçš„è¾“å‡ºå˜å›åºåˆ—æ ¼å¼ï¼Œä»¥ä¾›GRUä½¿ç”¨

        # å‡†å¤‡GRUçš„è¾“å…¥åºåˆ—
        # å°† (B * T, feat_dim) -> (B, T, feat_dim)
        gru_inputs_sequence = resnet_main_feat.view(B, T, -1) # -1 ä¼šè‡ªåŠ¨æ¨æ–­ä¸º feat_dim

        # æ•´ç†è¾…åŠ©ä»»åŠ¡çš„é¢„æµ‹åºåˆ—
        # å°† (B * T, 6) -> (B, T, 6)
        resnet_aux_predictions = resnet_aux_pred.view(B, T, -1) # -1 ä¼šè‡ªåŠ¨æ¨æ–­ä¸º 6

        # ç°åœ¨ gru_inputs_sequence å’Œ resnet_aux_predictions å°±æ˜¯ä½ æƒ³è¦çš„åºåˆ—å¼ é‡äº†
        # å¹¶ä¸”è¿™ä¸ªè¿‡ç¨‹æ¯” for å¾ªç¯å¿«å‡ ä¸ªæ•°é‡çº§ã€‚
        
        # GRUå¤„ç†æ•´ä¸ªåºåˆ—
        # å°† gru_inputs_sequence (å½¢çŠ¶ä¸º (B, T, C')) ä¼ é€’ç»™ self.gru æ—¶ï¼ŒPyTorch çš„ nn.GRU æ¨¡å—ä¼šåœ¨å†…éƒ¨è‡ªåŠ¨åœ°ã€é«˜æ•ˆåœ°å¾ªç¯ T æ¬¡ã€‚
        # æ¯æ¬¡å¾ªç¯ä¸­ï¼Œå®ƒä¼šå–å‡ºåºåˆ—ä¸­çš„ä¸€ä¸ªæ—¶é—´æ­¥ (t) çš„æ‰€æœ‰æ‰¹æ¬¡æ•°æ® (gru_inputs_sequence[:, t, :])ï¼Œå¹¶ä¸å½“å‰çš„éšè—çŠ¶æ€ä¸€èµ·ï¼Œè®¡ç®—å‡ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚
        # è¿™ä¸ªå†…éƒ¨å¾ªç¯æ˜¯é«˜åº¦ä¼˜åŒ–çš„ï¼Œé€šå¸¸é€šè¿‡ C++ æˆ– CUDA å®ç°ï¼Œæ¯” Python å¾ªç¯è¦é«˜æ•ˆå¾—å¤š
        # gru_output_sequence æ˜¯ GRU åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯éšè—çŠ¶æ€ï¼‰ã€‚å½¢çŠ¶æ˜¯ (B, T, gru_hidden_dim)ï¼ŒåŒ…å«äº†åºåˆ—ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€è¾“å‡ºã€‚
        # last_hidden_state æ˜¯ GRU æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶æ˜¯ (num_layers * num_directions, B, gru_hidden_dim)ã€‚
        # å¦‚æœæ˜¯å•å‘ GRUï¼Œåˆ™å½¢çŠ¶ä¸º (num_layers, B, gru_hidden_dim)ã€‚
        gru_output_sequence, last_hidden_state = self.gru(gru_inputs_sequence)
        
        # GRUçš„è¾“å‡ºåˆ†ä¸¤è·¯
        # GRUçš„è¾…åŠ©å¤´ï¼Œå¯¹æ¯ä¸€å¸§è¿›è¡Œæ˜¾å¼çš„é€Ÿåº¦/è§’é€Ÿåº¦é¢„æµ‹ï¼Œï¼ˆB,T,6ï¼‰
        gru_aux_predictions = self.gru_aux_head(gru_output_sequence)
        
        # æœ€ç»ˆçš„èåˆæ—¶ç©ºç‰¹å¾å‘é‡ (å–æœ€åä¸€å±‚çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€)
        final_feature_vector = last_hidden_state[-1, :, :]
        
        return final_feature_vector, resnet_aux_predictions, gru_aux_predictions

# Initialize Policy weights
def weights_init_(m,seed):
    torch.manual_seed(seed) #ä½¿ç”¨ä¼ å…¥çš„ç§å­
    if isinstance(m, nn.Linear): #åˆ¤æ–­æ¨¡å—æ˜¯ä¸æ˜¯çº¿æ€§å±‚ï¼Œæ˜¯çš„è¯å°±è¿›è¡Œåˆå§‹åŒ–
        torch.nn.init.xavier_uniform_(m.weight, gain=1)
        torch.nn.init.constant_(m.bias, 0)

def mlp(sizes, activation, output_activation=nn.Identity):
    layers = []
    for j in range(len(sizes)-1):
        act = activation if j < len(sizes)-2 else output_activation
        # è¿˜æ˜¯ä¼šæ‰§è¡Œn-1æ¬¡ï¼Œä½†å¾ªç¯æœ€åä¸€æ¬¡ï¼ˆj=n-2ï¼‰æ—¶æ¿€æ´»å‡½æ•°æ˜¯æ’ç­‰æ˜ å°„
        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]
    return nn.Sequential(*layers)
    # ç”Ÿæˆç½‘ç»œä¸”å…è®¸çµæ´»ä¿®æ”¹ï¼Œä½†å…¨éƒ½æ˜¯å…¨è¿æ¥å±‚ï¼Œå…¶ä¸­sizeå¯ä»¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æè¿°å¤§å°ï¼›åŒæ—¶jå’Œj+1åœ¨å¾ªç¯ä¸­è‡ªåŠ¨ç¡®ä¿ç›¸ä¹˜æ—¶è¡Œæ•°åˆ—æ•°ç›¸ç­‰
    # nn.Identity æ„å‘³ç€ç½‘ç»œçš„è¾“å‡ºå±‚å°†åº”ç”¨æ’ç­‰æ˜ å°„ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå³è¾“å‡ºå€¼ä¸è¾“å…¥å€¼å®Œå…¨ä¸€è‡´ï¼Œæ²¡æœ‰ç»è¿‡ä»»ä½•å˜æ¢
    # çµæ´»ç”¨æ˜Ÿå·è§£åŒ…
    # nn.Linear(a, b) ã€ä¸æ˜¯ä¸€ä¸ªå•çº¯çš„å…¨è¿æ¥å±‚ã€‘æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆlinear layerï¼‰çš„æ„é€ å‡½æ•°ã€‚å®ƒåˆ›å»ºäº†ä¸€ä¸ªå°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°è¾“å‡ºç‰¹å¾çš„çº¿æ€§å˜æ¢ã€‚
    # nn.Linear(a, b) æ¥å—è¡¨ç¤ºè¾“å…¥ç‰¹å¾çš„ç»´åº¦aå’Œè¾“å‡ºç‰¹å¾çš„ç»´åº¦bï¼Œçº¿æ€§å±‚çš„ä½œç”¨æ˜¯é€šè¿‡å­¦ä¹ ä¸€ç»„æƒé‡å’Œåç½®ï¼Œå°†è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾ã€‚
    # output = input * weight^T + bias
    # å…¶ä¸­ï¼Œinput æ˜¯è¾“å…¥ç‰¹å¾ï¼Œweight æ˜¯å½¢çŠ¶ä¸º (b, a) çš„æƒé‡çŸ©é˜µï¼Œbias æ˜¯å½¢çŠ¶ä¸º (b,) çš„åç½®é¡¹ã€‚^T è¡¨ç¤ºæƒé‡çŸ©é˜µçš„è½¬ç½®ã€‚

class GaussianPolicy(nn.Module):
    def __init__(self, embedding_dim, num_inputs, num_actions, hidden_sizes, 
                 activation, max_action, min_action, 
                 resnet_aux_outputs, gru_aux_outputs, 
                 gru_layers, dropout, RE_PARAMETERIZATION=True):
        super(GaussianPolicy, self).__init__()
        self.GRU = GRU(resnet_aux_outputs, embedding_dim, gru_aux_outputs, gru_layers=gru_layers, dropout=dropout)
        self.mlp_network=mlp([embedding_dim + num_inputs] + list(hidden_sizes), activation, activation) #ç‰¹å¾å‘é‡+ç›®æ ‡ä½ç½®+å¾€æœŸåŠ¨ä½œ
        self.mu_layer = nn.Linear(hidden_sizes[-1], num_actions)
        # ç”Ÿæˆmuçš„å±‚
        self.log_std_layer = nn.Linear(hidden_sizes[-1], num_actions)
        self.re_parameterization=RE_PARAMETERIZATION

        # åŠ¨ä½œç¼©æ”¾ï¼Œè¿™é‡Œåœ¨å¤–éƒ¨è§£å†³ï¼Œé¿å…åŠ¨ä½œç›¸å·®å¤ªå°
        self.action_scale = torch.FloatTensor([
                (max_action - min_action ) / 2.])
        self.action_bias = torch.FloatTensor([
                (max_action + min_action ) / 2.])

    def forward(self, img_sequence, state):
        # è¾“å…¥åˆ° GRU
        features, resnet_preds, gru_preds = self.GRU(img_sequence)  # æå–ç‰¹å¾å¼ é‡
        x=self.mlp_network(torch.cat([features,state],1))
        mean = self.mu_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)
        return mean, log_std, resnet_preds, gru_preds

    def sample(self, img_sequence, state):
        mean, log_std, resnet_output, gru_output = self.forward(img_sequence, state)
        std = torch.exp(log_std)
        normal = Normal(mean, std)
        x_t = normal.rsample()
        # ã€ä»¥ä¸‹æ–¹æ¡ˆæ˜¯ä»£ç ä½œè€…è‡ªå·±çš„æ–¹æ¡ˆï¼Œå…ˆå¾—åˆ°tanhåŠ¨ä½œå†å¯¹è¿™ä¸€åŠ¨ä½œæ±‚logã€‘
        y_t = torch.tanh(x_t) # æ²¡æœ‰åšé‡å‚æ•°åŒ–
        action = y_t * self.action_scale + self.action_bias #ä¸æ˜¯é‡å‚æ•°åŒ–ï¼Œåªæ˜¯å•çº¯æŠŠå€¼è°ƒæ•´åˆ°åŠ¨ä½œç©ºé—´èŒƒå›´å†…
        log_prob = normal.log_prob(x_t)
        # Enforcing Action Bound
        log_prob -= torch.log((1 - y_t.pow(2)) + epsilon) 
        # åŸè®ºæ–‡(21)å¼
        #log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon) #åŸè®ºæ–‡ä¸­å…¬å¼ï¼Œä½†æ˜¯å¤šäº†ä¸ªaction_scale
        log_prob = log_prob.sum(1, keepdim=True)
        mean = torch.tanh(mean) * self.action_scale + self.action_bias
        return action, log_prob, mean, resnet_output, gru_output # è¾…åŠ©å¤´è¾“å‡ºåˆ†åˆ«æ˜¯ï¼ˆB,T,9ï¼‰å’Œï¼ˆB,T,6ï¼‰

    def to(self, device):
        self.action_scale = self.action_scale.to(device)
        self.action_bias = self.action_bias.to(device)
        return super(GaussianPolicy, self).to(device)

class QNetwork(nn.Module):
    def __init__(self, num_inputs, num_actions, hidden_sizes,activation):
        super(QNetwork, self).__init__()
        #torch.manual_seed(42) #æ‰€æœ‰éšæœºæ•°ç§å­éƒ½ç”¨42
        # Q1 architecture
        self.Q_network_1=mlp([num_inputs+num_actions] + list(hidden_sizes)+[1], activation)

        # Q2 architecture
        self.Q_network_2=mlp([num_inputs+num_actions] + list(hidden_sizes)+[1], activation)

    def forward(self, state, action):
        xu = torch.cat([state, action], 1)
        x1 = self.Q_network_1(xu)
        x2 = self.Q_network_2(xu)
        return x1, x2